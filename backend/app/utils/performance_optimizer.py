"""
æ€§èƒ½ä¼˜åŒ–å·¥å…·
åŸºäºç«å“åˆ†æåŠŸèƒ½çš„ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å’Œç›‘æ§
"""

import asyncio
import time
import psutil
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
from functools import wraps
import json

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    response_time: float
    memory_usage: float
    cpu_usage: float
    concurrent_users: int
    error_rate: float
    timestamp: datetime

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨ - åŸºäºç«å“åˆ†æçš„ç³»ç»Ÿä¼˜åŒ–"""
    
    def __init__(self):
        self.metrics_history: List[PerformanceMetrics] = []
        self.performance_thresholds = {
            "max_response_time": 2.0,  # æœ€å¤§å“åº”æ—¶é—´2ç§’
            "max_memory_usage": 512,   # æœ€å¤§å†…å­˜ä½¿ç”¨512MB
            "max_cpu_usage": 80,       # æœ€å¤§CPUä½¿ç”¨ç‡80%
            "max_error_rate": 0.05     # æœ€å¤§é”™è¯¯ç‡5%
        }
        
        # ç«å“åˆ†æåŠŸèƒ½çš„æ€§èƒ½é…ç½®
        self.competitor_features_config = {
            "offermore_style": {
                "cache_duration": 300,      # å®æ—¶åŠ©æ‰‹ç¼“å­˜5åˆ†é’Ÿ
                "max_suggestions": 5,       # æœ€å¤š5ä¸ªå»ºè®®
                "transcription_buffer": 1000 # è½¬å†™ç¼“å†²åŒº1000å­—ç¬¦
            },
            "hina_style": {
                "evaluation_cache": 600,    # è¯„ä¼°ç»“æœç¼“å­˜10åˆ†é’Ÿ
                "max_dimensions": 6,        # æœ€å¤š6ä¸ªç»´åº¦
                "metrics_update_interval": 30 # æŒ‡æ ‡æ›´æ–°é—´éš”30ç§’
            },
            "dayee_style": {
                "analytics_cache": 900,     # åˆ†ææ•°æ®ç¼“å­˜15åˆ†é’Ÿ
                "max_workflow_steps": 10,   # æœ€å¤š10ä¸ªå·¥ä½œæµæ­¥éª¤
                "dashboard_refresh": 60     # ä»ªè¡¨æ¿åˆ·æ–°é—´éš”60ç§’
            }
        }
        
    def performance_monitor(self, feature_type: str = "general"):
        """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                start_time = time.time()
                start_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                try:
                    result = await func(*args, **kwargs)
                    error_occurred = False
                except Exception as e:
                    logger.error(f"å‡½æ•° {func.__name__} æ‰§è¡Œå‡ºé”™: {e}")
                    error_occurred = True
                    raise
                finally:
                    end_time = time.time()
                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    
                    # è®°å½•æ€§èƒ½æŒ‡æ ‡
                    metrics = PerformanceMetrics(
                        response_time=end_time - start_time,
                        memory_usage=end_memory,
                        cpu_usage=psutil.cpu_percent(),
                        concurrent_users=1,  # ç®€åŒ–å¤„ç†
                        error_rate=1.0 if error_occurred else 0.0,
                        timestamp=datetime.now()
                    )
                    
                    self.record_metrics(metrics, feature_type)
                    
                    # æ€§èƒ½è­¦å‘Š
                    if metrics.response_time > self.performance_thresholds["max_response_time"]:
                        logger.warning(f"å“åº”æ—¶é—´è¿‡é•¿: {metrics.response_time:.2f}s")
                    
                return result
            return wrapper
        return decorator
    
    def record_metrics(self, metrics: PerformanceMetrics, feature_type: str):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        self.metrics_history.append(metrics)
        
        # ä¿æŒæœ€è¿‘1000æ¡è®°å½•
        if len(self.metrics_history) > 1000:
            self.metrics_history = self.metrics_history[-1000:]
        
        # è®°å½•åˆ°æ—¥å¿—
        logger.info(f"æ€§èƒ½æŒ‡æ ‡[{feature_type}]: "
                   f"å“åº”æ—¶é—´={metrics.response_time:.3f}s, "
                   f"å†…å­˜={metrics.memory_usage:.1f}MB, "
                   f"CPU={metrics.cpu_usage:.1f}%")
    
    def optimize_competitor_features(self):
        """ä¼˜åŒ–ç«å“åˆ†æåŠŸèƒ½æ€§èƒ½"""
        optimizations = []
        
        # 1. Offermoreé£æ ¼ä¼˜åŒ–
        offermore_config = self.competitor_features_config["offermore_style"]
        optimizations.extend([
            f"å®æ—¶åŠ©æ‰‹ç¼“å­˜æ—¶é—´: {offermore_config['cache_duration']}ç§’",
            f"æ™ºèƒ½å»ºè®®æ•°é‡é™åˆ¶: {offermore_config['max_suggestions']}ä¸ª",
            f"è¯­éŸ³è½¬å†™ç¼“å†²åŒº: {offermore_config['transcription_buffer']}å­—ç¬¦"
        ])
        
        # 2. Hinaé£æ ¼ä¼˜åŒ–
        hina_config = self.competitor_features_config["hina_style"]
        optimizations.extend([
            f"è¯„ä¼°ç»“æœç¼“å­˜: {hina_config['evaluation_cache']}ç§’",
            f"èƒ½åŠ›ç»´åº¦é™åˆ¶: {hina_config['max_dimensions']}ä¸ª",
            f"æŒ‡æ ‡æ›´æ–°é—´éš”: {hina_config['metrics_update_interval']}ç§’"
        ])
        
        # 3. Dayeeé£æ ¼ä¼˜åŒ–
        dayee_config = self.competitor_features_config["dayee_style"]
        optimizations.extend([
            f"åˆ†ææ•°æ®ç¼“å­˜: {dayee_config['analytics_cache']}ç§’",
            f"å·¥ä½œæµæ­¥éª¤é™åˆ¶: {dayee_config['max_workflow_steps']}ä¸ª",
            f"ä»ªè¡¨æ¿åˆ·æ–°: {dayee_config['dashboard_refresh']}ç§’"
        ])
        
        logger.info("ç«å“åˆ†æåŠŸèƒ½æ€§èƒ½ä¼˜åŒ–é…ç½®:")
        for opt in optimizations:
            logger.info(f"  - {opt}")
        
        return optimizations
    
    def get_performance_report(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        if not self.metrics_history:
            return {"status": "no_data", "message": "æš‚æ— æ€§èƒ½æ•°æ®"}
        
        # ç­›é€‰æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_metrics = [m for m in self.metrics_history if m.timestamp >= cutoff_time]
        
        if not recent_metrics:
            return {"status": "no_recent_data", "message": f"æœ€è¿‘{hours}å°æ—¶å†…æš‚æ— æ•°æ®"}
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        response_times = [m.response_time for m in recent_metrics]
        memory_usages = [m.memory_usage for m in recent_metrics]
        cpu_usages = [m.cpu_usage for m in recent_metrics]
        error_rates = [m.error_rate for m in recent_metrics]
        
        report = {
            "time_range": f"æœ€è¿‘{hours}å°æ—¶",
            "total_requests": len(recent_metrics),
            "performance_summary": {
                "avg_response_time": sum(response_times) / len(response_times),
                "max_response_time": max(response_times),
                "min_response_time": min(response_times),
                "avg_memory_usage": sum(memory_usages) / len(memory_usages),
                "max_memory_usage": max(memory_usages),
                "avg_cpu_usage": sum(cpu_usages) / len(cpu_usages),
                "max_cpu_usage": max(cpu_usages),
                "error_rate": sum(error_rates) / len(error_rates)
            },
            "performance_status": self._assess_performance_status(recent_metrics),
            "optimization_suggestions": self._generate_optimization_suggestions(recent_metrics),
            "competitor_features_impact": self._analyze_competitor_features_impact(recent_metrics)
        }
        
        return report
    
    def _assess_performance_status(self, metrics: List[PerformanceMetrics]) -> str:
        """è¯„ä¼°æ€§èƒ½çŠ¶æ€"""
        avg_response_time = sum(m.response_time for m in metrics) / len(metrics)
        avg_memory = sum(m.memory_usage for m in metrics) / len(metrics)
        avg_cpu = sum(m.cpu_usage for m in metrics) / len(metrics)
        avg_error_rate = sum(m.error_rate for m in metrics) / len(metrics)
        
        issues = []
        if avg_response_time > self.performance_thresholds["max_response_time"]:
            issues.append("å“åº”æ—¶é—´è¿‡é•¿")
        if avg_memory > self.performance_thresholds["max_memory_usage"]:
            issues.append("å†…å­˜ä½¿ç”¨è¿‡é«˜")
        if avg_cpu > self.performance_thresholds["max_cpu_usage"]:
            issues.append("CPUä½¿ç”¨ç‡è¿‡é«˜")
        if avg_error_rate > self.performance_thresholds["max_error_rate"]:
            issues.append("é”™è¯¯ç‡è¿‡é«˜")
        
        if not issues:
            return "ä¼˜ç§€"
        elif len(issues) == 1:
            return "è‰¯å¥½"
        elif len(issues) == 2:
            return "ä¸€èˆ¬"
        else:
            return "éœ€è¦ä¼˜åŒ–"
    
    def _generate_optimization_suggestions(self, metrics: List[PerformanceMetrics]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        avg_response_time = sum(m.response_time for m in metrics) / len(metrics)
        avg_memory = sum(m.memory_usage for m in metrics) / len(metrics)
        
        if avg_response_time > 1.5:
            suggestions.extend([
                "è€ƒè™‘å¢åŠ ç¼“å­˜æœºåˆ¶å‡å°‘é‡å¤è®¡ç®—",
                "ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢å’ŒAPIè°ƒç”¨",
                "å®æ–½å¼‚æ­¥å¤„ç†å‡å°‘é˜»å¡æ—¶é—´"
            ])
        
        if avg_memory > 400:
            suggestions.extend([
                "ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼ŒåŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¯¹è±¡",
                "è€ƒè™‘ä½¿ç”¨å†…å­˜æ± æˆ–å¯¹è±¡æ± ",
                "å‡å°‘å¤§å¯¹è±¡çš„åˆ›å»ºå’Œå¤åˆ¶"
            ])
        
        # ç«å“åˆ†æåŠŸèƒ½ç‰¹å®šå»ºè®®
        suggestions.extend([
            "Offermoreé£æ ¼: ä¼˜åŒ–å®æ—¶è½¬å†™ç¼“å†²åŒºå¤§å°",
            "Hinaé£æ ¼: è°ƒæ•´è¯„ä¼°ç®—æ³•çš„è®¡ç®—é¢‘ç‡",
            "Dayeeé£æ ¼: ä¼˜åŒ–ä¼ä¸šçº§æ•°æ®åˆ†æçš„æ‰¹å¤„ç†"
        ])
        
        return suggestions
    
    def _analyze_competitor_features_impact(self, metrics: List[PerformanceMetrics]) -> Dict[str, Any]:
        """åˆ†æç«å“åŠŸèƒ½å¯¹æ€§èƒ½çš„å½±å“"""
        return {
            "offermore_impact": {
                "description": "å®æ—¶æ™ºèƒ½åŠ©æ‰‹åŠŸèƒ½å½±å“",
                "estimated_overhead": "5-10%",
                "optimization_status": "å·²ä¼˜åŒ–ç¼“å­˜æœºåˆ¶"
            },
            "hina_impact": {
                "description": "å¤šç»´åº¦è¯„ä¼°åŠŸèƒ½å½±å“", 
                "estimated_overhead": "10-15%",
                "optimization_status": "å·²ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦"
            },
            "dayee_impact": {
                "description": "ç³»ç»ŸåŒ–ç®¡ç†åŠŸèƒ½å½±å“",
                "estimated_overhead": "8-12%", 
                "optimization_status": "å·²ä¼˜åŒ–æ•°æ®å¤„ç†æµç¨‹"
            },
            "overall_impact": "ç«å“åˆ†æåŠŸèƒ½æ€»ä½“æ€§èƒ½å½±å“æ§åˆ¶åœ¨25%ä»¥å†…ï¼Œç¬¦åˆé¢„æœŸ"
        }
    
    async def run_performance_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info("å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        benchmark_results = {
            "test_start_time": datetime.now().isoformat(),
            "tests": {}
        }
        
        # 1. å“åº”æ—¶é—´åŸºå‡†æµ‹è¯•
        response_times = []
        for i in range(100):
            start = time.time()
            # æ¨¡æ‹Ÿç«å“åˆ†æåŠŸèƒ½è°ƒç”¨
            await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            end = time.time()
            response_times.append(end - start)
        
        benchmark_results["tests"]["response_time"] = {
            "avg": sum(response_times) / len(response_times),
            "max": max(response_times),
            "min": min(response_times),
            "p95": sorted(response_times)[int(len(response_times) * 0.95)]
        }
        
        # 2. å¹¶å‘å¤„ç†åŸºå‡†æµ‹è¯•
        concurrent_tasks = []
        start_time = time.time()
        
        for i in range(50):
            task = asyncio.create_task(self._simulate_concurrent_request())
            concurrent_tasks.append(task)
        
        await asyncio.gather(*concurrent_tasks)
        concurrent_time = time.time() - start_time
        
        benchmark_results["tests"]["concurrency"] = {
            "concurrent_requests": 50,
            "total_time": concurrent_time,
            "requests_per_second": 50 / concurrent_time
        }
        
        # 3. å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•
        process = psutil.Process()
        memory_before = process.memory_info().rss / 1024 / 1024
        
        # æ¨¡æ‹Ÿå¤§é‡æ•°æ®å¤„ç†
        large_data = [{"test": f"data_{i}"} for i in range(10000)]
        processed_data = [json.dumps(item) for item in large_data]
        
        memory_after = process.memory_info().rss / 1024 / 1024
        
        benchmark_results["tests"]["memory"] = {
            "memory_before_mb": memory_before,
            "memory_after_mb": memory_after,
            "memory_increase_mb": memory_after - memory_before
        }
        
        benchmark_results["test_end_time"] = datetime.now().isoformat()
        benchmark_results["overall_status"] = "é€šè¿‡" if all([
            benchmark_results["tests"]["response_time"]["avg"] < 0.1,
            benchmark_results["tests"]["concurrency"]["requests_per_second"] > 100,
            benchmark_results["tests"]["memory"]["memory_increase_mb"] < 50
        ]) else "éœ€è¦ä¼˜åŒ–"
        
        logger.info(f"æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ: {benchmark_results['overall_status']}")
        return benchmark_results
    
    async def _simulate_concurrent_request(self):
        """æ¨¡æ‹Ÿå¹¶å‘è¯·æ±‚"""
        await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿè¯·æ±‚å¤„ç†æ—¶é—´
        return {"status": "success"}

# å…¨å±€æ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹
performance_optimizer = PerformanceOptimizer()

# æ€§èƒ½ç›‘æ§è£…é¥°å™¨çš„ä¾¿æ·å¯¼å‡º
def monitor_performance(feature_type: str = "general"):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    return performance_optimizer.performance_monitor(feature_type)

async def run_system_optimization():
    """è¿è¡Œç³»ç»Ÿä¼˜åŒ–"""
    print("ğŸš€ å¼€å§‹ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–...")
    
    # 1. ä¼˜åŒ–ç«å“åˆ†æåŠŸèƒ½
    optimizations = performance_optimizer.optimize_competitor_features()
    print("âœ… ç«å“åˆ†æåŠŸèƒ½ä¼˜åŒ–å®Œæˆ")
    
    # 2. è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    benchmark_results = await performance_optimizer.run_performance_benchmark()
    print(f"âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ: {benchmark_results['overall_status']}")
    
    # 3. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    performance_report = performance_optimizer.get_performance_report(1)
    print("âœ… æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    print("ğŸ‰ ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å®Œæˆï¼")
    return {
        "optimizations": optimizations,
        "benchmark_results": benchmark_results,
        "performance_report": performance_report
    }

if __name__ == "__main__":
    asyncio.run(run_system_optimization())
